{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65SMeAPipnLX"
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "!pip3 install graphviz\n",
    "!pip3 install dask\n",
    "!pip install \"dask[complete]\" \n",
    "!pip3 install toolz\n",
    "!pip3 install cloudpickle\n",
    "\n",
    "#Code References- \n",
    "# https://github.com/dask/dask-tutorial\n",
    "# https://github.com/dask/dask-tutorial/blob/master/07_dataframe.ipynb\n",
    "import dask.dataframe as dd#similar to pandas\n",
    "\n",
    "import pandas as pd#pandas to create small dataframes \n",
    "\n",
    "!pip3 install folium\n",
    "# if this doesnt work refere install_folium.JPG in drive\n",
    "import folium #open street map\n",
    "\n",
    "# unix time: https://www.unixtimestamp.com/\n",
    "import datetime #Convert to unix time\n",
    "\n",
    "import time #Convert to unix time\n",
    "\n",
    "\n",
    "import numpy as np#Do aritmetic operations on arrays\n",
    "\n",
    "# matplotlib: used to plot graphs\n",
    "import matplotlib\n",
    "# matplotlib.use('nbagg') : matplotlib uses this protocall which makes plots more user intractive like zoom in and zoom out\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns#Plots\n",
    "from matplotlib import rcParams#Size of plots  \n",
    "\n",
    "# this lib is used while we calculate the stight line distance between two (lat,lon) pairs in miles\n",
    "!pip install gpxpy\n",
    "import gpxpy.geo #Get the haversine distance\n",
    "\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# download migwin: https://mingw-w64.org/doku.php/download/mingw-builds\n",
    "# install it in your system and keep the path, migw_path ='installed path'\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "# to install xgboost: pip3 install xgboost\n",
    "# if it didnt happen check install_xgboost.JPG\n",
    "import xgboost as xgb\n",
    "\n",
    "# to install sklearn: pip install -U scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoV1TT2qpYVa"
   },
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN4agQdupYVb"
   },
   "source": [
    "### Train-Test Split\n",
    "Before we start predictions using the tree based regression models we take 3 months of 2016 pickup data and split it such that for every region we have 70% data in train and 30% in test, ordered date-wise for every region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN-XFT2vQnhH"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiVsOBxYpYVb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing data to be split into train and test, The below prepares data in cumulative form which will be later split into test and train\n",
    "# number of 10min indices for jan 2015= 24*31*60/10 = 4464\n",
    "# number of 10min indices for jan 2016 = 24*31*60/10 = 4464\n",
    "# number of 10min indices for feb 2016 = 24*29*60/10 = 4176\n",
    "# number of 10min indices for march 2016 = 24*31*60/10 = 4464\n",
    "# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n",
    "# that are happened for three months in 2016 data\n",
    "\n",
    "# print(len(regions_cum))\n",
    "# 40\n",
    "# print(len(regions_cum[0]))\n",
    "# 13104\n",
    "\n",
    "# we take number of pickups that are happened in last 5 10min intravels\n",
    "number_of_time_stamps = 5\n",
    "\n",
    "# output varaible\n",
    "# it is list of lists\n",
    "# it will contain number of pickups 13099 for each cluster\n",
    "output = []\n",
    "\n",
    "\n",
    "# tsne_lat will contain 13104-5=13099 times lattitude of cluster center for every cluster\n",
    "# Ex: [[cent_lat 13099times],[cent_lat 13099times], [cent_lat 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "tsne_lat = []\n",
    "\n",
    "\n",
    "# tsne_lon will contain 13104-5=13099 times logitude of cluster center for every cluster\n",
    "# Ex: [[cent_long 13099times],[cent_long 13099times], [cent_long 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "tsne_lon = []\n",
    "\n",
    "# we will code each day \n",
    "# sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n",
    "# for every cluster we will be adding 13099 values, each value represent to which day of the week that pickup bin belongs to\n",
    "# it is list of lists\n",
    "tsne_weekday = []\n",
    "\n",
    "# its an numpy array, of shape (523960, 5)\n",
    "# each row corresponds to an entry in out data\n",
    "# for the first row we will have [f0,f1,f2,f3,f4] fi=number of pickups happened in i+1th 10min intravel(bin)\n",
    "# the second row will have [f1,f2,f3,f4,f5]\n",
    "# the third row will have [f2,f3,f4,f5,f6]\n",
    "# and so on...\n",
    "tsne_feature = []\n",
    "\n",
    "\n",
    "tsne_feature = [0]*number_of_time_stamps\n",
    "for i in range(0,40):\n",
    "    tsne_lat.append([kmeans.cluster_centers_[i][0]]*13099)\n",
    "    tsne_lon.append([kmeans.cluster_centers_[i][1]]*13099)\n",
    "    # jan 1st 2016 is thursday, so we start our day from 4: \"(int(k/144))%7+4\"\n",
    "    # our prediction start from 5th 10min intravel since we need to have number of pickups that are happened in last 5 pickup bins\n",
    "    tsne_weekday.append([int(((int(k/144))%7+4)%7) for k in range(5,4464+4176+4464)])\n",
    "    # regions_cum is a list of lists [[x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], .. 40 lsits]\n",
    "    tsne_feature = np.vstack((tsne_feature, [regions_cum[i][r:r+number_of_time_stamps] for r in range(0,len(regions_cum[i])-number_of_time_stamps)]))\n",
    "    output.append(regions_cum[i][5:])\n",
    "tsne_feature = tsne_feature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "TFMimQdx2X1b",
    "outputId": "f97eb0c5-8370-4cec-c106-07d4e567fce0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa3ac30625ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtsne_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tsne_feature' is not defined"
     ]
    }
   ],
   "source": [
    "tsne_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2LABmZp24a8",
    "outputId": "ac596c56-d770-44c9-bd0f-4e7372c94e9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4459"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lCkxAm9_zEa0",
    "outputId": "9304c785-364a-4607-dc9d-7fd78fb61258"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tsne_feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMv1rC51pYVc",
    "outputId": "57d5fe08-1c5c-455d-fcf9-c9547f4fa8dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tsne_lat[0])*len(tsne_lat) == tsne_feature.shape[0] == len(tsne_weekday)*len(tsne_weekday[0]) == 40*13099 == len(output)*len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofQ8zCtgpYVe"
   },
   "outputs": [],
   "source": [
    "# Getting the predictions of exponential moving averages to be used as a feature in cumulative form\n",
    "\n",
    "# upto now we computed 8 features for every data point that starts from 50th min of the day\n",
    "# 1. cluster center lattitude\n",
    "# 2. cluster center longitude\n",
    "# 3. day of the week \n",
    "# 4. f_t_1: number of pickups that are happened previous t-1th 10min intravel\n",
    "# 5. f_t_2: number of pickups that are happened previous t-2th 10min intravel\n",
    "# 6. f_t_3: number of pickups that are happened previous t-3th 10min intravel\n",
    "# 7. f_t_4: number of pickups that are happened previous t-4th 10min intravel\n",
    "# 8. f_t_5: number of pickups that are happened previous t-5th 10min intravel\n",
    "\n",
    "# from the baseline models we said the exponential weighted moving avarage gives us the best error\n",
    "# we will try to add the same exponential weighted moving avarage at t as a feature to our data\n",
    "# exponential weighted moving avarage => p'(t) = alpha*p'(t-1) + (1-alpha)*P(t-1) \n",
    "alpha=0.3\n",
    "\n",
    "# it is a temporary array that store exponential weighted moving avarage for each 10min intravel, \n",
    "# for each cluster it will get reset\n",
    "# for every cluster it contains 13104 values\n",
    "predicted_values=[]\n",
    "\n",
    "# it is similar like tsne_lat\n",
    "# it is list of lists\n",
    "# predict_list is a list of lists [[x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], .. 40 lsits]\n",
    "predict_list = []\n",
    "tsne_flat_exp_avg = []\n",
    "for r in range(0,40):\n",
    "    for i in range(0,13104):\n",
    "        if i==0:\n",
    "            predicted_value= regions_cum[r][0]\n",
    "            predicted_values.append(0)\n",
    "            continue\n",
    "        predicted_values.append(predicted_value)\n",
    "        predicted_value =int((alpha*predicted_value) + (1-alpha)*(regions_cum[r][i]))\n",
    "    predict_list.append(predicted_values[5:])\n",
    "    predicted_values=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvIFa9-PpYVf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31AvW-MApYVf",
    "outputId": "c54ba6a9-f28b-4932-f60c-204f4cb3a4e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data : 9169\n",
      "size of test data : 3929\n"
     ]
    }
   ],
   "source": [
    "# train, test split : 70% 30% split\n",
    "# Before we start predictions using the tree based regression models we take 3 months of 2016 pickup data \n",
    "# and split it such that for every region we have 70% data in train and 30% in test,\n",
    "# ordered date-wise for every region\n",
    "print(\"size of train data :\", int(13099*0.7))\n",
    "print(\"size of test data :\", int(13099*0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ty3ywNTepYVh"
   },
   "outputs": [],
   "source": [
    "# extracting first 9169 timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "train_features =  [tsne_feature[i*13099:(13099*i+9169)] for i in range(0,40)]\n",
    "# temp = [0]*(12955 - 9068)\n",
    "test_features = [tsne_feature[(13099*(i))+9169:13099*(i+1)] for i in range(0,40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9T0p_qhipYVi",
    "outputId": "717d41c5-5ca5-419e-8ddb-4369e43cf055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data clusters 40 Number of data points in trian data 9169 Each data point contains 5 features\n",
      "Number of data clusters 40 Number of data points in test data 3930 Each data point contains 5 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in trian data\", len(train_features[0]), \"Each data point contains\", len(train_features[0][0]),\"features\")\n",
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in test data\", len(test_features[0]), \"Each data point contains\", len(test_features[0][0]),\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUrc4FPepYVk"
   },
   "outputs": [],
   "source": [
    "# extracting first 9169 timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "tsne_train_flat_lat = [i[:9169] for i in tsne_lat]\n",
    "tsne_train_flat_lon = [i[:9169] for i in tsne_lon]\n",
    "tsne_train_flat_weekday = [i[:9169] for i in tsne_weekday]\n",
    "tsne_train_flat_output = [i[:9169] for i in output]\n",
    "tsne_train_flat_exp_avg = [i[:9169] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJ4h3S58pYVl"
   },
   "outputs": [],
   "source": [
    "# extracting the rest of the timestamp values i.e 30% of 12956 (total timestamps) for our test data\n",
    "tsne_test_flat_lat = [i[9169:] for i in tsne_lat]\n",
    "tsne_test_flat_lon = [i[9169:] for i in tsne_lon]\n",
    "tsne_test_flat_weekday = [i[9169:] for i in tsne_weekday]\n",
    "tsne_test_flat_output = [i[9169:] for i in output]\n",
    "tsne_test_flat_exp_avg = [i[9169:] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3_aVVZnpYVm"
   },
   "outputs": [],
   "source": [
    "# the above contains values in the form of list of lists (i.e. list of values of each region), here we make all of them in one list\n",
    "train_new_features = []\n",
    "for i in range(0,40):\n",
    "    train_new_features.extend(train_features[i])\n",
    "test_new_features = []\n",
    "for i in range(0,40):\n",
    "    test_new_features.extend(test_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CacNoTj8pYVo"
   },
   "outputs": [],
   "source": [
    "# converting lists of lists into sinle list i.e flatten\n",
    "# a  = [[1,2,3,4],[4,6,7,8]]\n",
    "# print(sum(a,[]))\n",
    "# [1, 2, 3, 4, 4, 6, 7, 8]\n",
    "\n",
    "tsne_train_lat = sum(tsne_train_flat_lat, [])\n",
    "tsne_train_lon = sum(tsne_train_flat_lon, [])\n",
    "tsne_train_weekday = sum(tsne_train_flat_weekday, [])\n",
    "tsne_train_output = sum(tsne_train_flat_output, [])\n",
    "tsne_train_exp_avg = sum(tsne_train_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAHbGnkopYVs"
   },
   "outputs": [],
   "source": [
    "# converting lists of lists into sinle list i.e flatten\n",
    "# a  = [[1,2,3,4],[4,6,7,8]]\n",
    "# print(sum(a,[]))\n",
    "# [1, 2, 3, 4, 4, 6, 7, 8]\n",
    "\n",
    "tsne_test_lat = sum(tsne_test_flat_lat, [])\n",
    "tsne_test_lon = sum(tsne_test_flat_lon, [])\n",
    "tsne_test_weekday = sum(tsne_test_flat_weekday, [])\n",
    "tsne_test_output = sum(tsne_test_flat_output, [])\n",
    "tsne_test_exp_avg = sum(tsne_test_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5ND-T1apYVt",
    "outputId": "bf84cd12-ee66-49c2-a662-d70dfc87632a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366760, 9)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "columns = ['ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "df_train = pd.DataFrame(data=train_new_features, columns=columns) \n",
    "df_train['lat'] = tsne_train_lat\n",
    "df_train['lon'] = tsne_train_lon\n",
    "df_train['weekday'] = tsne_train_weekday\n",
    "df_train['exp_avg'] = tsne_train_exp_avg\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa8LZ28GpYVw",
    "outputId": "91d77071-61f2-4c63-e2d3-4474b7d403f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157200, 9)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "df_test = pd.DataFrame(data=test_new_features, columns=columns) \n",
    "df_test['lat'] = tsne_test_lat\n",
    "df_test['lon'] = tsne_test_lon\n",
    "df_test['weekday'] = tsne_test_weekday\n",
    "df_test['exp_avg'] = tsne_test_exp_avg\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "eQj1LfMnpYVx",
    "outputId": "505c2d04-fa6d-4a11-d0c3-f4f4ebdc5e4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft_5</th>\n",
       "      <th>ft_4</th>\n",
       "      <th>ft_3</th>\n",
       "      <th>ft_2</th>\n",
       "      <th>ft_1</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>weekday</th>\n",
       "      <th>exp_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143</td>\n",
       "      <td>145</td>\n",
       "      <td>119</td>\n",
       "      <td>113</td>\n",
       "      <td>124</td>\n",
       "      <td>40.776228</td>\n",
       "      <td>-73.982119</td>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>119</td>\n",
       "      <td>113</td>\n",
       "      <td>124</td>\n",
       "      <td>121</td>\n",
       "      <td>40.776228</td>\n",
       "      <td>-73.982119</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>113</td>\n",
       "      <td>124</td>\n",
       "      <td>121</td>\n",
       "      <td>131</td>\n",
       "      <td>40.776228</td>\n",
       "      <td>-73.982119</td>\n",
       "      <td>4</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113</td>\n",
       "      <td>124</td>\n",
       "      <td>121</td>\n",
       "      <td>131</td>\n",
       "      <td>110</td>\n",
       "      <td>40.776228</td>\n",
       "      <td>-73.982119</td>\n",
       "      <td>4</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124</td>\n",
       "      <td>121</td>\n",
       "      <td>131</td>\n",
       "      <td>110</td>\n",
       "      <td>116</td>\n",
       "      <td>40.776228</td>\n",
       "      <td>-73.982119</td>\n",
       "      <td>4</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ft_5  ft_4  ft_3  ft_2  ft_1        lat        lon  weekday  exp_avg\n",
       "0   143   145   119   113   124  40.776228 -73.982119        4      121\n",
       "1   145   119   113   124   121  40.776228 -73.982119        4      120\n",
       "2   119   113   124   121   131  40.776228 -73.982119        4      127\n",
       "3   113   124   121   131   110  40.776228 -73.982119        4      115\n",
       "4   124   121   131   110   116  40.776228 -73.982119        4      115"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rykiaxYlpYVz"
   },
   "source": [
    "### Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4-FX3CdpYV0"
   },
   "outputs": [],
   "source": [
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "\n",
    "# some of methods of LinearRegression()\n",
    "# fit(X, y[, sample_weight])\tFit linear model.\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict using the linear model\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# set_params(**params)\tSet the parameters of this estimator.\n",
    "# -----------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1-2-copy-8/\n",
    "# -----------------------\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_reg=LinearRegression().fit(df_train, tsne_train_output)\n",
    "\n",
    "y_pred = lr_reg.predict(df_test)\n",
    "lr_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = lr_reg.predict(df_train)\n",
    "lr_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WslHxeoTGRIv"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename='/content/drive/MyDrive/nyc_linreg.pkl'\n",
    "pickle.dump(lr_reg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbdQSLMipYV1"
   },
   "source": [
    "### Using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yH4SyUqpYV1",
    "outputId": "9661710c-a5ca-4bd0-b3bf-e0d2b609b4ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=4,\n",
       "                      min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=40, n_jobs=-1, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned random forest regressor on our train data\n",
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# apply(X)\tApply trees in the forest to X, return leaf indices.\n",
    "# decision_path(X)\tReturn the decision path in the forest\n",
    "# fit(X, y[, sample_weight])\tBuild a forest of trees from the training set (X, y).\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict regression target for X.\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "regr1 = RandomForestRegressor(max_features='sqrt',min_samples_leaf=4,min_samples_split=3,n_estimators=40, n_jobs=-1)\n",
    "regr1.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDEoraMPKysJ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename='/content/drive/MyDrive/nyc_rfreg.pkl'\n",
    "pickle.dump(lr_reg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB0zPii6pYV3"
   },
   "outputs": [],
   "source": [
    "# Predicting on test data using our trained random forest model \n",
    "\n",
    "# the models regr1 is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = regr1.predict(df_test)\n",
    "rndf_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = regr1.predict(df_train)\n",
    "rndf_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAw-LW-PpYV5",
    "outputId": "75befe16-208c-42a6-b0c1-376fd7d0c75a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ft_5', 'ft_4', 'ft_3', 'ft_2', 'ft_1', 'lat', 'lon', 'weekday',\n",
      "       'exp_avg'],\n",
      "      dtype='object')\n",
      "[0.03753423 0.08174161 0.12113021 0.16669707 0.23527788 0.00265597\n",
      " 0.00287077 0.00196441 0.35012785]\n"
     ]
    }
   ],
   "source": [
    "#feature importances based on analysis using random forest\n",
    "print (df_train.columns)\n",
    "print (regr1.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p0IhKaSpYV6"
   },
   "source": [
    "### Using XgBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nP3gp-0kpYV7",
    "outputId": "5dce2f14-ec73-4d8c-d06a-338779c48de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:56:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=3, missing=None, n_estimators=1000,\n",
       "             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=200, reg_lambda=200, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 150,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n",
    "\n",
    "# find more about XGBRegressor function here http://xgboost.readthedocs.io/en/latest/python/python_api.html?#module-xgboost.sklearn\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# xgboost.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', \n",
    "# booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, \n",
    "# colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, \n",
    "# missing=None, **kwargs)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n",
    "# get_score(importance_type='weight') -> get the feature importance\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "x_model = xgb.XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=3,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " reg_alpha=200, reg_lambda=200,\n",
    " colsample_bytree=0.8,nthread=4)\n",
    "x_model.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTuNZOc6pYV_"
   },
   "outputs": [],
   "source": [
    "#predicting with our trained Xg-Boost regressor\n",
    "# the models x_model is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = x_model.predict(df_test)\n",
    "xgb_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = x_model.predict(df_train)\n",
    "xgb_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3U0SLHApYWA",
    "outputId": "06204cb3-aa85-4860-e22f-13f1afaa7dba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_avg': 881,\n",
       " 'ft_1': 1096,\n",
       " 'ft_2': 954,\n",
       " 'ft_3': 815,\n",
       " 'ft_4': 783,\n",
       " 'ft_5': 1131,\n",
       " 'lat': 495,\n",
       " 'lon': 574,\n",
       " 'weekday': 173}"
      ]
     },
     "execution_count": 157,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature importances\n",
    "x_model.get_booster().get_score(importance_type='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8AqXZM_PS3L"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename='/content/drive/MyDrive/nyc_xgbreg.pkl'\n",
    "pickle.dump(lr_reg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN_F4XI9pYWB"
   },
   "source": [
    "### Calculating the error metric values for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxWASeeEpYWB"
   },
   "outputs": [],
   "source": [
    "train_mape=[]\n",
    "test_mape=[]\n",
    "\n",
    "train_mape.append((mean_absolute_error(tsne_train_output,df_train['ft_1'].values))/(sum(tsne_train_output)/len(tsne_train_output)))\n",
    "train_mape.append((mean_absolute_error(tsne_train_output,df_train['exp_avg'].values))/(sum(tsne_train_output)/len(tsne_train_output)))\n",
    "train_mape.append((mean_absolute_error(tsne_train_output,rndf_train_predictions))/(sum(tsne_train_output)/len(tsne_train_output)))\n",
    "train_mape.append((mean_absolute_error(tsne_train_output, xgb_train_predictions))/(sum(tsne_train_output)/len(tsne_train_output)))\n",
    "train_mape.append((mean_absolute_error(tsne_train_output, lr_train_predictions))/(sum(tsne_train_output)/len(tsne_train_output)))\n",
    "\n",
    "test_mape.append((mean_absolute_error(tsne_test_output, df_test['ft_1'].values))/(sum(tsne_test_output)/len(tsne_test_output)))\n",
    "test_mape.append((mean_absolute_error(tsne_test_output, df_test['exp_avg'].values))/(sum(tsne_test_output)/len(tsne_test_output)))\n",
    "test_mape.append((mean_absolute_error(tsne_test_output, rndf_test_predictions))/(sum(tsne_test_output)/len(tsne_test_output)))\n",
    "test_mape.append((mean_absolute_error(tsne_test_output, xgb_test_predictions))/(sum(tsne_test_output)/len(tsne_test_output)))\n",
    "test_mape.append((mean_absolute_error(tsne_test_output, lr_test_predictions))/(sum(tsne_test_output)/len(tsne_test_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVAsA4-IpYWC",
    "outputId": "fe334a24-448f-4cfc-f9be-0b513dbbddf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Metric Matrix (Tree Based Regression Methods) -  MAPE\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model -                             Train:  0.14870666996426116       Test:  0.14225522601041551\n",
      "Exponential Averages Forecasting -           Train:  0.14121603560900353       Test:  0.13490049942819257\n",
      "Linear Regression -                         Train:  0.1385989096704736       Test:  0.13287222822006217\n",
      "Random Forest Regression -                   Train:  0.0988037051619016      Test:  0.13336744964196395\n"
     ]
    }
   ],
   "source": [
    "print (\"Error Metric Matrix (Tree Based Regression Methods) -  MAPE\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model -                             Train: \",train_mape[0],\"      Test: \",test_mape[0])\n",
    "print (\"Exponential Averages Forecasting -           Train: \",train_mape[1],\"      Test: \",test_mape[1])\n",
    "print (\"Linear Regression -                         Train: \",train_mape[3],\"      Test: \",test_mape[3])\n",
    "print (\"Random Forest Regression -                   Train: \",train_mape[2],\"     Test: \",test_mape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsWghvyVpYWE"
   },
   "source": [
    "### Error Metric Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajqLUxkjpYWE",
    "outputId": "5854a627-0331-4148-969d-63e2e5b8e544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Metric Matrix (Tree Based Regression Methods) -  MAPE\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model -                             Train:  0.14870666996426116       Test:  0.14225522601041551\n",
      "Exponential Averages Forecasting -           Train:  0.14121603560900353       Test:  0.13490049942819257\n",
      "Linear Regression -                         Train:  0.14212750303572363       Test:  0.1348928075901918\n",
      "Random Forest Regression -                   Train:  0.0988037051619016      Test:  0.13336744964196395\n",
      "XgBoost Regression -                         Train:  0.1385989096704736       Test:  0.13287222822006217\n",
      "--------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"Error Metric Matrix (Tree Based Regression Methods) -  MAPE\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model -                             Train: \",train_mape[0],\"      Test: \",test_mape[0])\n",
    "print (\"Exponential Averages Forecasting -           Train: \",train_mape[1],\"      Test: \",test_mape[1])\n",
    "print (\"Linear Regression -                         Train: \",train_mape[4],\"      Test: \",test_mape[4])\n",
    "print (\"Random Forest Regression -                   Train: \",train_mape[2],\"     Test: \",test_mape[2])\n",
    "print (\"XgBoost Regression -                         Train: \",train_mape[3],\"      Test: \",test_mape[3])\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyaxAeTTSFx9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx_3kAU7uoJl"
   },
   "source": [
    "# Pickle file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9dbR2KSurI0"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename='/content/drive/MyDrive/nyc_xgbreg.pkl'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zCqniUDyuwAb",
    "outputId": "bd6f4875-6c36-44e5-a463-ef379403e69f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0.22.2.post1'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.__getstate__()['_sklearn_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4gtHOeKFweMc",
    "outputId": "086ae211-c082-4496-dabf-a75d5ef2396c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0.22.2.post1'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "filename='/content/drive/MyDrive/nyc_rfreg.pkl'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.__getstate__()['_sklearn_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNoBbS7-qj3c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Regression modelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
